{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Inference of Local trianed LLMA2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "# NOTE: FOLLOWING PIPELINE TAKING TOO MUCH MEMORY AND GETTING OUT OF MEMORY ERROR SO USING TRANSFORMERS APPROACH TO LOAD\n",
    "#from transformers import pipeline\n",
    "##pipe = pipeline(\"text-generation\", model=\"krishnareddy/llama2-sft\",  device=0)\n",
    "#text = f\"Question: What is Newton 3rd law?\"\n",
    "#pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f181ca4f5f5431e8bb1f1c9448c9df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#From Huggingface or Local model use any one one of these\n",
    "hf_model_repo=\"krishnareddy/llama2-sft\"\n",
    "local_model_repo=\"results/final_merged_checkpoint\"\n",
    "\n",
    "\n",
    "device_map={\"\": 0}\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_repo)\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_repo, load_in_4bit=True,  torch_dtype=torch.float16, device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Question: Who invloved in 2nd world war? \\n\\n Answer:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# Run the model to infere an output\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=500, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Inference of original LLAMA2 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#From Huggingface or Local model use any one one of these\n",
    "hf_model_repo=\"meta-llama/Llama-2-7b-hf\"\n",
    "device_map={\"\": 0}\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_repo)\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model_repo, load_in_4bit=True,  torch_dtype=torch.float16, device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Who invloved in 2nd world war?\n",
      "\n",
      "Generated instruction:\n",
      "\n",
      "The Axis Powers were Germany, Italy, and Japan. The Allies were the United States, Great Britain, France, and the Soviet Union.\n",
      "Who was involved in the Second World War?\n",
      "The Axis Powers were Germany, Italy, and Japan. The Allies were the United States, Great Britain, France, and the Soviet Union. The United States, Great Britain, and France were the Allies in World War II. The Soviet Union was not an ally until after the German invasion of the Soviet Union in 1941.\n",
      "Who was involved in the Second World War and why?\n",
      "The Axis Powers were Germany, Italy, and Japan. The Allies were the United States, Great Britain, France, and the Soviet Union. The United States, Great Britain, and France were the Allies in World War II. The Soviet Union was not an ally until after the German invasion of the Soviet Union in 1941. The Allies were the United States, Great Britain, France, and the Soviet Union. The Axis Powers were Germany, Italy, and Japan.\n",
      "Who was involved in the Second World War and why? The Axis Powers were Germany, Italy, and Japan. The Allies were the United States, Great Britain, France, and the Soviet Union. The United States, Great Britain, and France were the Allies in World War II. The Soviet Union was not an ally until after the German invasion of the Soviet Union in 1941. The Axis Powers were Germany, Italy, and Japan. The Allies were the United States, Great Britain, France, and the Soviet Union. The United States, Great Britain, and France were the Allies in World War II. The Soviet Union was not an ally until after the German invasion of the Soviet Union in 1941. The Axis Powers were Germany, Italy, and Japan. The Allies were the United States, Great Britain, France, and the Soviet Union. The United States, Great Britain, and France were the Allies in World War II. The Soviet Union was not an ally until after the German invasion of the Soviet Union in 1941. The Axis Powers were Germany, Italy, and Japan. The Allies were the United States, Great Britain, France, and the Soviet Union. The United States, Great\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Who invloved in 2nd world war?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# Run the model to infere an output\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=500, do_sample=True, top_p=0.9,temperature=0.5)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Prompt:\\n{prompt}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
